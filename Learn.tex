\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{nips_2017}
\usepackage{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\usepackage[style=apa,backend=biber]{biblatex}
\addbibresource{NIPS2017.bib}
\DeclareLanguageMapping{english}{american-apa}

\title{Enhancing exploration and learning through intrinsic motivation: A combination of novelty seeking and hierarchical reinforcement learning}

\author{
  Maria K.~Eckstein \\
  Department of Psychology \\
  UC Berkeley \\
  Berkeley, CA 94720 \\
  \texttt{maria.eckstein@berkeley.edu} \\  
  \And
  Anne Collins \\
  UC Berkeley \\
  Address \\
  \texttt{annecollins@berkeley.edu} \\
  \And
  Tom Griffiths \\
  UC Berkeley \\
  Address \\
  \texttt{tom_griffiths@berkeley.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
  We propose a novel task paradigm, which aims to differentiate reward-learning from novelty-learning, and thereby explains how structured, goal-directed behavior can arise in complex environment that are sparse in rewards. 
\end{abstract}


\section{Introduction}

It is desirable for a complex cognitive system, like a human, to understand its environment in a way that allows prediction of future states and events. This is because with the ability to predict the results of its own actions, an agent will be able to control its environment, creating the states and events it desires. The prediction and control of future rewards, for example, has been formalized in reinforcement learning theory (\cite{sutton_reinforcement_2017}). In this framework, an agent learns over time how much (cumulative, temporally discounted) future reward to expect, given a current state of the environment and the agent's action policy. Given the ability to predict, the agent can then maximize future rewards by adjusting its action policy.

\begin{itemize}
	\item But it's not all about rewards: We want to learn the structure of the environment (predictive model) before we know which states and actions will be rewarding.
	\item Example: When an infant looks at a computer monitor, she probably sees a dark frame, filled with colored rectangles and blinking, moving blobs of various sizes. A computer-savy adult, on the other hand, sees a Mac or Windows machine, a bug in the code, or the need to download a new plugin. The perception of the computer monitor changes dramatically upon interaction with it. Interaction creates a structured model of the computer.
	\item Other examples: language learning: learn phonemes, syllables, words, 2-word sentences, 3-word sentences, whole sentences, poetry; motor learning: learn to grasp, hold, manipulate, ..., play the violin.
	\item Indeed, psychological research of the last century has stressed again and again that humans represent their world hierarchically (Chess study - chunking; old research on expertise; TS; Anderson? Making coffee)
	\item What would a predictive model look like that is not focused on reward? Instead of a flat table that specifies how much reward to expect from each state-action pair, we would have a collection of policies that specify how to reach various states in the environment. The agent can control the environment by selecting whatever sub-goal it likes at the moment. Once rewards are added, it can use the learned structure to maximize rewards. 
	\item introduce HRL
	\item ML / RL advances in this direction: Machado, ... (cite papers that talk about exploration; sparse rewards; old 2-step paper; Schmidhuber; Deepak Pathak; the other papers in dropbox papers and in the old draft)
	\item The big question is now how to find valuable sub-goals. Much research, mostly about bottleneck states in the state space (Botvinick -> check in my powerpoint). But this is unrealistic: an agent does not have access to all the information necessary to calculate which states are bottleneck states. 
	\item The goal of this project is to create an algorithm with human-like behavior. Valuable sug-goals will be identified using the agent's curiosity, i.e., intrinsic motivation (Deepak Pathak's citations; Botvinick paper? -> check powerpoint)
\end{itemize}


\section{Methods}

\subsection{Novelty-based hierarchical RL}

\begin{itemize}
	\item The agent explores an environment with sparse rewards, with the goal of "understanding" it (understand = predict \& control). The environment produces events in response to the agent's actions, according to certain rules (see below).
	\item The agent is intrinsically motivated to investigate novel events (curiosity). The occurrence of a novel event therefore triggers the creation of a sub-goal and the learning of a policy $\pi_e$ targeted at producing $e$. 
	\item Once the agent decides to investigate a specific event $e$, its actions are guided by the event policy $\pi_e$ that is aimed at producing $e$ ("option policy" in HRL framework). Event policies $\pi$ are learned over time through interactions with the environment.
	\item This means that there are two fundamental ways of selecting actions: Based on curiosity, and based on even policies.
	\item Note: The algo is tabular: Discrete states and actions, no function approximation using ANNs.
\end{itemize}

\subsubsection{Action selection based on curiosity}

\begin{itemize}
	\item The agent's curiosity $c_e$ for a specific event $e$ is determined by the novelty $n_e$ of the event and the agent's learning rate $\alpha$. $n_e$ is a function of $i_e$, the number of times $e$ has occurred, and the rate $\lambda$ at which novelty decays for a given agent. $n_e$ is weighted by $\gamma^{j_e}$, whereby $\gamma$ is the agent's tendency to discount future rewards, and $j_e$ is the number of time steps spent within $\pi_e$ (see below). Taken together: $c_e = c_e + \alpha_c (n_e - c_e)$ and $n_e = \gamma^{j_e} e^{-\lambda i_e}$. (Note: Agents can only be curious about events that they have seen before, i.e., that have occurred at least once.)
	\item The agent selects events using an $\epsilon$-greedy policy: In $1-\epsilon$ \% of cases, it selects the event that it is most curious about; in $\epsilon$ \% of cases, it randomly selects among all other events.
	\item Once the agent selects a specific event, it uses the corresponding event policy $\pi_e$ for actions selection (see below). After $\pi_e$ terminates (usually when $e$ occurs), $i_e$ is incremented by one (if $e$ occurred), $j_e$ represents the number of moves spent inside the option, and $c_e$ is updated. The subsequent event is selected based on the agent's updated curiosity. 
\end{itemize}

\subsubsection{Action selection based on event policy}

\begin{itemize}
	\item All option policies $\pi$ are $\epsilon$-greedy action selection policies based on event-specific Q-values. Each $\pi_e$ has a separate set of Q-values, representing the value of each state-action pair for achieving event $e$. 
	\item States are defined in terms of previous events, encoded by features $\phi$ (one feature for each action in the event option's repertoire, signifying whether the event corresponding to this action has occurred in the previous trial).
	\item Actions: Crucially, the agent constructs event policies hierarchically. All event policies (except the most basic ones) are based on more fundamental event options, not basic actions.
	\item Event-specific Q-values are learned through standard TD learning on the weights $\theta$ of features $\phi$: $\theta = \theta + \alpha (r + \gamma V' - V)$, whereby $V = \theta^T \phi$ and $V' = \theta^T \phi'$.
	\item $\pi_e$ terminates either when the target event $e$ occurs ($r=1$), or when the agent gives up without $e$ occurring ($r=0$). An agent's propensity to terminate options without achieving the sub-goal is controlled by the distraction parameter, which determines the probability of terminating an option at each step of option execution. 
	\item Note: $\pi_e$ is updated each time $e$ occurs, even when $\pi_e$ was not in charge of action selection. This is to make it more realistic and more similar to humans, equipped with working memory.
\end{itemize}

\subsubsection{Comparison agents}

We compared the novelty-based hierarchical RL agent to several other RL agents. The reward-based flat RL agent did not set sub-goals or create event policies. It selected actions based on reward, rather then novelty: $v = v + \alpha (r - v)$. Here, $r$ is the number of events occurring at a given time point. 

The two remaining comparison agents are combinations of the reward-based flat RL agent and the novelty-based hierarchical RL agent: The reward-based hierarchical RL agent, and the novelty-based flat RL agent.


\subsection{The task}

We aimed to create a task that is hierarchical in nature and that can easily be presented to human participants in a laboratory setting. The goal was to reproduce the structure of more natural hierarchical learning tasks, such as the ones mentioned above. 

In the task, the agent can execute a small number of basic actions and the environment responds with specific events, according to the following rules. Each basic action leads to one specific "level-0" event. The sequential execution of $n$ basic actions leads to one specific "level-1" event. Similarly, the sequential occurrence of $n$ level-1 events triggers the occurrence of one specific level-2 event, and so on. 

This structure is aimed at mirroring naturalistic phenomena, such as the combination of phonemes into syllables, syllables into words, words into sentences, etc. Note that the environment is strictly hierarchical in that events at level only depend on the events at the level right below (which depend on events at the level below, etc.).

More details about our specific implementation: $n$ is identical at all levels; the number of possible events is identical at all levels; rules are allowed to overlap, but cannot be totally identical). E.g., one rule might be [01] (event 0 followed by event 1), and another might be [21].

Example: [Table in the same shape as the state / value tables; each cell contains two numbers that specify the two events that need to occur at the level below in order to produce this event. Shows the rules of the example below.]

\section{Results}

All results shown for agents with $\alpha = 0.3$, $\lambda = 0.3$, $\gamma = 0.7$, $\epsilon = 0.2$, $distraction = 0.1$. All environments had 3 basic actions, and 3 events at each level of the hierarchy. Each event was triggered by the sequential occurrence of 2 specific events at the level below. The environment had 6 levels of hierarchy in an effort to max out the agents. 

\subsection{Behavior of the novelty-based hierarchical agent}

\begin{itemize}
	\item Novelty over time: One plot for each level, showing how novelty decreases over time; will be the average of 5 agents who played the same environment [the environment with the rules above].
	\item Final option policies: Final Q-value tables for a few options (one at each level?), averaged over the 5 agents.
	\item Learning policies over time: Change in RMSE over trials, plotted separately for each level (averaging RMSE within levels)
\end{itemize}

\subsection{Comparison of the novelty-based hierarchical agent and other agents}

\begin{itemize}
	\item Number of events per trial over time
	\item Number of discovered events over time
	\item Number of perseverative actions over time (perseveration = same action at least twice in a row)
	\item Size of "DA bursts" over time ("DA burst" = RPE at either novelty or theta level)
	\item Number of times actions are executed, by number of times an action is part of a higher-level event (0, 1, or 2)
\end{itemize}

\section{Conclusion}

We have shown that a novelty-based hierarchical RL agent
\begin{itemize}
	\item explores an environment more thoroughly than a flat agent (discovers more events)
	\item selects actions in a more balanced way (fewer perseverations)
	\item "gets bored" over time, especially  for very simple tasks (decreasing DA bursts; decrease in novelty scales with level)
\end{itemize}

This makes sense and might be similar to humans.

In our notion, the agent learns to "understand" its environment. It is able to predict events and control them according to its own wishes.

This is interesting because usually people think about prediction in terms of model-based (versus model-free) methods, rather than hierarchical methods. But creating hierarchy in this way really is a way to achieve prediction. By creating an event option policy, the agent learns to predict the event. By selecting the option policy, it controls the environment.

"Embodiment" is really important, the possibility of the agent to interact with its environment. Because understanding implies control. Kids love to experiment. Cite stuff on how experimentation helps kids learn? (Fei Shu, Tania Lombrozo?, Alison Gopnik?; active learning literature)

\subsection{Future directions}

\begin{itemize}
	\item Algo itself could be extended: function approximation (neural network) rather than tabular RL; model curiosity in a more sophisticated way: based on prediction rather than event count (Deepak Pathak's citations)
	\item Bring the task to humans and compare
	\item extract RL features and look for them in the brain
	\item Check if we can reproduce specific phenomena related to hierarchical learning. First, does the agent have a learning advantage when it is allowed to explore the environment first (without rewards), before asked to exploit it (add rewards, see how long it takes the agent to find the best strategy, compared to a flat or non-trained agent). Second, is there interference when the agent is trained in one environment and then posed into an environment whose options are just slightly different (reason why second-language speakers never get rid of their accent: it's situated at a low level in the language hierarchy).
	\item Check if there is more on the powerpoint.
\end{itemize}

Taken together, by creating the hierarchical novelty-seeking agent, we hope to show that learning of meaningful behavior can be driven by learning itself, rather than external rewards. In this framework, the learner produces the crucial learning signals itself, by selecting actions that over time reduce novelty. In other words, this learner balances the search for novelty with reducing prediction errors. It explores the environment in a systematic way, forms "hypotheses" and conducts "experiments", in order to learn more. 

\section*{References}

\printbibliography

\end{document}
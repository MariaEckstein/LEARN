\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{nips_2017}
\usepackage{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\usepackage[style=apa,backend=biber]{biblatex}
\addbibresource{NIPS2017.bib}
\DeclareLanguageMapping{english}{american-apa}

\title{Structuring exploration and enhancing learning through intrinsic motivation: A combination of novelty seeking and hierarchical reinforcement learning}

\author{
  Maria K.~Eckstein \\
  Department of Psychology \\
  UC Berkeley \\
  Berkeley, CA 94720 \\
  \texttt{maria.eckstein@berkeley.edu} \\  
  \And
  Anne Collins \\
  UC Berkeley \\
  Address \\
  \texttt{annecollins@berkeley.edu} \\
  \And
  Tom Griffiths \\
  UC Berkeley \\
  Address \\
  \texttt{tom_griffiths@berkeley.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
  We propose an algorithm that combines curiosity (intrinsic motivation to explore novel states) and temporal abstraction (creation of option policies at multiple levels of abstraction), with the aim of shedding some light on how structured, goal-directed behavior can arise in complex environment that are sparse in rewards, as shown by humans. The algorithm shows meaningful behavior when exploring a complex environment. The eventual goal is to bring it to humans and see if they do something similar and look into their brains.
\end{abstract}


\section{Introduction}

Humans have the astonishing ability to learn from few and unorganized samples in an environment of sparse rewards, and to create goal-directed action plans in novel, unseen situations, with incredibly quick computation times. How is this possible?

Research in the cognitive sciences over the past decades has suggested that at least two components are crucial for such behavior: intelligent exploration, driven by intrinsic motivation and targeted hypothesis testing (Gopnik, Lombrozo, Schmidhuber, painting paper, Deepak's citations); and hierarchical structure of the representation of the environment or task at hand (Anderson, Collins, Chess study, Miller \& Cohen, 2001, Badre \& Frank).

We argue that these two components, intrinsically-motivated exploration and hierarchical representation, are more closely related than is usually acknowledged. Implemented jointly, they can lead to an understanding of the environment that goes beyond and is more flexible than, say, the mere prediction of reward as in traditional RL frameworks (Sutton \& Barto). We argue that by combining intrinsically-motivated exploration with hierarchy, agents can discover meaningful patterns of behavior without explicit training, and learn to form multi-step plans without the need of a predictive model. Related work has suggested that this is a valuable approach (Work on novelty: Deepak Pathak, Pulkit Arawal, Yael Niv, Schmidhuber; on hierarchy: Marlos Machado, Botvinick, Yael Niv, Pedro Tsividis; combination of both: Deep HRL paper)

We propose an algorithm that explores an environment of sparse rewards based on its intrinsic curiosity, which is triggered by novel events. Once an event triggers the agent's curiosity (imagine an infant playing with a toy that suddenly makes a new sound), the agent will try to reproduce the event (try to reproduce the sound). Crucially, the agent does not know yet how to produce the event that it is curious about. In order to achieve its goal, it therefore has to learn, through trial and error, a specific option policy that leads to the event. In this way, the agent will learn policies to control various aspects of its environment. We model the process of option creation through hierarchical reinforcement learning, relying on the options framework (Sutton, Precup, Singh). Crucially, the only learning signal hereby is the agent's internal reward upon achieving its self-selected goal.

Over the long term, option policies become better and better at producing their goal events, such that the corresponding events can be produced more frequently and, over time, decrease in novelty. This leads to a decrease in the agent's curiosity, and its intrinsic motivation to explore events further once they are understood. Nevertheless, the world is full of wonders. Whereas an infant might be curious about individual sounds, a toddler might be interested in children's songs, and an adult in twelve-tone music or Italian Opera. Acquiring some skills, rather than reducing motivation and slowing down exploration, opens up new possibilities of learning even more abstract skills. In our framework, basic actions are the building blocks for only the most fundamental option policies. More abstract option policies can be created by using option policies as the building blocks; this process can be repeated indefinitely, resulting in option policies of increasing abstraction, limited only by the temporal horizon of the agent. In other words, the agent can learn to achieve abstract goals by breaking them down into sub-goals and sub-sub-goals, etc.


\section{Methods}

We aimed to create a task that is hierarchical in nature and that could easily be presented to human participants in a laboratory setting. The goal was to create an environment with a strictly hierarchical structure, as found in more naturalistic circumstances such as language or motor learning. Language learning, for example, progresses from learning phonemes to combining phonemes into syllables, syllables into words, words into phrases, phrases into sentences, etc.

\subsection{The task: States, actions, and transitions}

In our task, an agent can execute a small number of "basic actions" and the environment responds with specific events, according to the following rules. Each basic action deterministically produces a specific "basic event", e.g., executing action $a_{0, 3}$ produces event $e_{0, 3}$. In addition, certain sequences of basic actions trigger "level-1" events, e.g., executing $a_{0, 3}$ then $a_{0, 2}$ might produce event $e_{1, 2}$. Similarly, certain sequences of level-1 events produce level-2 events (the occurrence of $e_{1, 2}$ then $e_{1, 4}$ might produce $e_{2, 1}$), and so on.

Formally, let $A_0 = a_{0, 1}, a_{0, 2}, \ldots, a_{0, k}$ be the set of the agent's $k$ basic actions and let $S = (F_{0, 1}, F_{0, 2}, \ldots, F_{0, k}, F_{1, 1}, F_{1, 2}, \ldots, F_{1, j}, F_{2, 1}, \ldots)$ be the parameterized state space defined by binary features $f_{l, i}$ referring to level $l$ and index $i$.

The transition function between the initial state $s_0$ and $s'$ upon execution of $a_{0, i}, i \in [1, \ldots, k]$ is then the transition from $s_0 = (f_{l, i} = 0) \forall l, i$ to $s' = (f_{0, i} = 1, all other f_{x, y} = 0)$. In words, action $a_{0, i}$ changes a single feature of the environment from 0 to 1, feature $f_{0, i}$. This is what we will refer to as an "event": the transition from a state in which $f_{l, i} = 0$ to a state in which $f_{l, i} = 1 \forall l, i$. The first part of the transition function is therefore the fact that every basic action $a_{0, i}$ produces a basic event $e_{0, i}$, which defines the difference between the old state $s$ and the new state $s'$. (Note that we only allow a single feature per level to be 1 and force all other features at this level to be 0.)

The transition function has the following additional property. A small selection of state-action combinations deterministically produces additional, higher-level events. The identity of these state-action combinations and their results varies from game to game. For example, starting in state $s' = (f_{0, i} = 1, all other f_{x, y} = 0)$ and executing action $a_{0, j}$, might lead to the event $e_{1, m}$ (in addition to $e_{0, j}$), depending on the rules of the current game. Features $f_{l_i}$ keep track of previous events at level $l$.

By defining features this way, the environment has the Markov property even though its workings are defined by sequences of events at varying time scales. 


\subsection{Event novelty and curiosity}

The agent is exploring such an environment based on its intrinsic curiosity. The number of times each event has occurred determines the event's novelty. Event novelty determines the agent's curiosity about the event, which in turn determines the agent's propensity to select the event as a target of exploration, i.e., to set the sub-goal of reproducing the event.

Formally, novelty $n_e = e^{-\lambda i_e}$; $\lambda$ is the agent's rate of novelty decay (in the experiments shown below, $\lambda = 0.3$) and $i_e$ is the number times event $e$ has occurred since the agent's first encounter with the environment.
Curiosity $c_e$ is updated according to $c_e = c_e + \alpha (\gamma^{j_e} n_e - c_e)$ whenever event $e$ occurs; $\alpha$ is the agent's learning rate ($\alpha = 0.3$); $\gamma$ is the agent's discounting of the future ($\gamma = 0.9$), and $j_e$ is the number of time steps spent within option $o_e$, which led to event $e$ ($j_e = 0$ when event $e$ happened without executing option $o_e$; $j_e = 1$ when $e$ is a basic event).
In order to select specific events as sub-goals, the agent uses $\epsilon$-greedy selection based on its curiosity.

\subsection{Option creation}

Initially, the agent possesses only "basic actions", i.e., it can only select basic events as sub-goals. When a higher-level event occurs for the first time, it is added to the agent's option space as a potential sub-goal. The option policy of the new sub-goal can use all options at the level below to reach the new sub-goal.

Formally, the agent's initial option space $O$ comprises only basic actions, $O = A_0$. When event $e_{l, i}$ occurs for the first time, the agent's option space is extended by the option $o_{e_{l, i}}$ targeted at producing event $e_{l, i}$: $O = O \cup o_{e_{l, i}}$. Each option $o_e$ has the whole state space as initiation set, $I = S$. The termination probability $\beta = 1$ for all states in which $f_e = 1$, i.e., in which the target event $e$ occurred, and $\beta = 0.1$ in all other states. The action space of $o_{e_{l, i}}$ comprises all options $o_{e_{l-i, j}}$ at the level below. For example, if $o_1$ is a level-1 option, its action space comprises all basic actions $a_{0, i} \in A_0$. 

\subsection{Learning option policies}

When the agent selects a specific event $e$ as sub-goal, the option policy $o_e$ starts controlling action selection. The values of the option policy are updated in each step through TD learning, and eventually reflect which actions lead to the sub-goal. After the termination of an option, the agent's curiosity about the sub-goal is updated based on the novelty of the produced event (or the failure to produce the event).

Formally, option $o$ has the option policy $\pi_o$, which is the $\epsilon$-greedy action selection based on option values $V_o$. $V_o$ are defined by the weights $\theta$ of the binary features $\phi$ that describe the state of the environment, $V_o = \theta^T \phi$. In our case, the features indicate which events have happened previously (see below for details).
$V_o$ are updated through temporal difference learning on the weights $\theta$: $\theta = \theta + \alpha (r + \gamma max_a(V_o') - V_o(a))$; $V_o' = \theta^T \phi'$; the pseudo-reward $r = 1$ when $e$ occurred (and the option terminates) and $r = 0$ otherwise; $V_o' = 0$ when the option terminates. 

\subsection{Final remarks about the agent and environment}

Because options are strictly hierarchical in this way, time scales differ between options at different levels. Level-1 option policies are updated at each time step (because the agent executes one basic action at each time step), but level-2 option policies are only updated after a chosen level-1 option terminates. In the experiments presented below, all events are triggered by the sequential occurrence of two events at the level below. Therefore level-2 option policies can at most be updated every other time step, level-3 options every fourth, level-4 option every 8th, and so on.

For the experiment presented below, we produced games / environments with the following features: $l = 6$; $k = 5$ for all levels. The choice of 6 levels of hierarchy was in an effort to see when the algos fail - it's supposed to be hard. All rules about the creation of higher-level events had length 2, i.e., two subsequent event at one level could create an event at the level above; rules were thereby allowed to overlap, but could not be identical, e.g., when one rule was $e_{l_2} + e_{l_3} \rightarrow e_{l+1, 1}$ ($e_{l_2}$ followed by $e_{l_3}$ produces $e_{l+1, 1}$), another rule might be $e_{l_2} + e_{l_4} \rightarrow e_{l+1, 2}$).

All agents had the following parameter settings: $\alpha = 0.3$, $\lambda = 0.3$, $\gamma = 0.9$, and $\epsilon = 0.2$. 

Note: The current algorithm is tabular, i.e., all states, events, and actions are discrete, and state-action values were represented as tables, rather than approximated through neural networks.

\subsection{Comparison agents}

We compared the novelty-based hierarchical agent described in the paragraphs above to three other agents. The reward-based hierarchical agent employed options in the same way as the novelty-based hierarchical agent, but used reward $r$ instead of novelty as an update signal for option values, $Q(s, o) = Q(s, o) + \alpha (r - Q(s, o))$. We defined reward as the number events produced in a given trial, i.e., the number of features $f_{l, i}$ that changed from 0 to 1. 

The novelty-based flat agent selected options based on curiosity like the novelty-based hierarchical agent, but it did not create or use abstract options. Therefore, $O = A_0$ and $S = S_0$. 

The reward-based flat agent is a combination of these two agents.

\section{Results}

\subsection{Behavior of the novelty-based hierarchical agent}

Single environment, average over 11 agents.

Rules of this particular environment:

\begin{center}
 \begin{tabular}{|c | c c c c c|} 
 \hline
 Level & Rule 1 & Rule 2 & Rule 3 & Rule 4 & Rule 5 \\ [0.5ex] 
 \hline
 0 & [$0 \rightarrow 0$] & [$1 \rightarrow 1$] & [$2 \rightarrow 2$] & [$3 \rightarrow 3$] & [$4 \rightarrow 4$] \\ 
 \hline
 1 & [$2 + 0 \rightarrow 0$] & [$4 + 2 \rightarrow 1$] & [$1 + 2 \rightarrow 2$] & [$2 + 1 \rightarrow 3$] & [$4 + 3 \rightarrow 4$] \\ 
 \hline
 2 & [$1 + 3 \rightarrow 0$] & [$2 + 3 \rightarrow 1$] & [$0 + 2 \rightarrow 2$] & [$3 + 1 \rightarrow 3$] & [$4 + 3 \rightarrow 4$] \\ 
 \hline
 3 & [$3 + 2 \rightarrow 0$] & [$4 + 1 \rightarrow 1$] & [$2 + 1 \rightarrow 2$] & [$2 + 3 \rightarrow 3$] & [$1 + 4 \rightarrow 4$] \\ 
 \hline
 4 & [$1 + 3 \rightarrow 0$] & [$0 + 2 \rightarrow 1$] & [$3 + 2 \rightarrow 2$] & [$4 + 3 \rightarrow 3$] & [$0 + 3 \rightarrow 4$] \\ 
 \hline
 \end{tabular}
\end{center}

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{data/plots/ANoveltyCuriosity.png}
	\caption{Change in event novelty (A) and curiosity (B) over exploration time. The higher-level an event, the slower it becomes boring. When novelty = 1, the corresponding event has not been discovered yet.}
	\label{TaskFigure}
\end{figure}

\subsection{Comparison of the novelty-based hierarchical agent and other agents}

Averages over 16 different environments, 3 agents per environment. All effects hold when looking at just one environment.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{data/plots/CValues.png}
	\caption{The four agents' option values over time. Each agent has its individual pattern.}
	\label{TaskFigure}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{data/plots/CDiscEvents.png}
	\caption{Number of events discovered by each of the four agents. Both novelty and hierarchy contribute to exploration success, such that the novelty-based hierarchical agent outperforms the other three.}
	\label{TaskFigure}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{data/plots/CFracEvents.png}
	\caption{Numbers of events per 10 trials, by level (level 0 would always be 1 and is not shown), relative to theoretical maximum.}
	\label{TaskFigure}
\end{figure}

\section{Conclusion}

We have shown that a novelty-based hierarchical RL agent explores an environment more thoroughly than a flat agent (discovers more events). Doing this, it does not get bored - very simple action sequences (options) get boring, but it keeps adding abstract goals, which get less and less boring. 

This makes sense and might be similar to humans.

The agent learns to "understand" its environment. It is able to predict events and control them according to its own wishes.

This is interesting because usually people think about prediction in terms of model-based (versus model-free) methods, rather than hierarchical methods. But creating hierarchy in this way really is a way to achieve prediction. By creating an event option policy, the agent learns to predict the event. By selecting the option policy, it controls the environment.

"Embodiment" is really important, the possibility of the agent to interact with its environment. Because understanding implies control. Kids love to experiment. Cite stuff on how experimentation helps kids learn? (Fei Shu, Tania Lombrozo?, Alison Gopnik?; active learning literature)

\subsection{Future directions}

\begin{itemize}
	\item Algo itself could be extended: function approximation (neural network) rather than tabular RL; model curiosity in a more sophisticated way: based on prediction rather than event count (Deepak Pathak's citations)
	\item Bring the task to humans and compare
	\item extract RL features and look for them in the brain
	\item Check if we can reproduce specific phenomena related to hierarchical learning. First, does the agent have a learning advantage when it is allowed to explore the environment first (without rewards), before asked to exploit it (add rewards, see how long it takes the agent to find the best strategy, compared to a flat or non-trained agent). Second, is there interference when the agent is trained in one environment and then posed into an environment whose options are just slightly different (reason why second-language speakers never get rid of their accent: it's situated at a low level in the language hierarchy).
	\item Check if there is more on the powerpoint.
	\item Maybe all is really just exploration: Science, in the end, is the task of understanding the world.
\end{itemize}

Taken together, by creating the hierarchical novelty-seeking agent, we hope to show that learning of meaningful behavior can be driven by learning itself, rather than external rewards. In this framework, the learner produces the crucial learning signals itself, by selecting actions that over time reduce novelty. In other words, this learner balances the search for novelty with reducing prediction errors. It explores the environment in a systematic way, forms "hypotheses" and conducts "experiments", in order to learn more. 

\subsection{Old Intro}

It is desirable for a complex cognitive system, like a human, to understand its environment in a way that allows prediction of future states and events. This is because with the ability to predict the results of its own actions, an agent will be able to control its environment, creating the states and events it desires. The prediction and control of future rewards, for example, has been formalized in reinforcement learning theory (\cite{sutton_reinforcement_2017}). In this framework, an agent learns over time how much (cumulative, temporally discounted) reward to expect in the future, given the current state of the environment and the agent's action policy. Given the ability to predict, the agent can then maximize future rewards by adjusting its action policy.

\begin{itemize}
	\item But it's not all about rewards: We want to learn the structure of the environment (predictive model) before we know which states and actions will be rewarding.
	\item Example: When an infant looks at a computer monitor, she probably sees a dark frame, filled with colored rectangles and blinking, moving blobs of various sizes. A computer-savy adult, on the other hand, sees a Mac or Windows machine, a bug in the code, or the need to download a new plugin. The perception of the computer monitor changes dramatically upon interaction with it. Interaction creates a structured model of the computer.
	\item Other examples: language learning: learn phonemes, syllables, words, 2-word sentences, 3-word sentences, whole sentences, poetry; motor learning: learn to grasp, hold, manipulate, ..., play the violin.
	\item Indeed, psychological research of the last century has stressed again and again that humans represent their world hierarchically (Chess study - chunking; old research on expertise; TS; Anderson? Making coffee)
	\item What would a predictive model look like that is not focused on reward? Instead of a flat table that specifies how much reward to expect from each state-action pair, we would have a collection of policies that specify how to reach various states in the environment. The agent can control the environment by selecting whatever sub-goal it likes at the moment. Once rewards are added, it can use the learned structure to maximize rewards. 
	\item introduce HRL
	\item ML / RL advances in this direction: Machado, ... (cite papers that talk about exploration; sparse rewards; old 2-step paper; Schmidhuber; Deepak Pathak; the other papers in dropbox papers and in the old draft)
	\item The big question is now how to find valuable sub-goals. Much research, mostly about bottleneck states in the state space (Botvinick -> check in my powerpoint). But this is unrealistic: an agent does not have access to all the information necessary to calculate which states are bottleneck states. 
	\item The goal of this project is to create an algorithm with human-like behavior. Valuable sug-goals will be identified using the agent's curiosity, i.e., intrinsic motivation (Deepak Pathak's citations; Botvinick paper? -> check powerpoint)
\end{itemize}

\printbibliography

\end{document}
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{nips_2017}
\usepackage{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\usepackage[style=apa,backend=biber]{biblatex}
\addbibresource{NIPS2017.bib}
\DeclareLanguageMapping{english}{american-apa}

\title{Enhancing exploration and learning through intrinsic motivation: A combination of novelty seeking and hierarchical reinforcement learning}

\author{
  Maria K.~Eckstein \\
  Department of Psychology \\
  UC Berkeley \\
  Berkeley, CA 94720 \\
  \texttt{maria.eckstein@berkeley.edu} \\  
  \And
  Anne Collins \\
  UC Berkeley \\
  Address \\
  \texttt{annecollins@berkeley.edu} \\
  \And
  Tom Griffiths \\
  UC Berkeley \\
  Address \\
  \texttt{tom_griffiths@berkeley.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
  We propose an algorithm that combines curiosity (intrinsic motivation to explore novel states) and temporal abstraction (creation of option policies at multiple levels of abstraction), with the aim of shedding some light on how structured, goal-directed behavior can arise in complex environment that are sparse in rewards, as shown by humans. The algorithm shows meaningful behavior when exploring a complex environment. The eventual goal is to bring it to humans and see if they do something similar and look into their brains.
\end{abstract}


\section{Introduction}

Humans have the astonishing ability to learn from few and unorganized samples in an environment of sparse rewards, and to create goal-directed action plans in novel, unseen situations, with incredibly quick computation times. How is this possible?

Research in the cognitive sciences over the past decades has suggested that at least two components are crucial for such behavior: intelligent exploration, driven by intrinsic motivation and targeted hypothesis testing (Gopnik, Lombrozo, Schmidhuber, painting paper, Deepak's citations); and hierarchical structure of the representation of the environment or task at hand (Anderson, Collins, Chess study, Miller \& Cohen, 2001, Badre \& Frank).

We argue that these two components, intrinsically-motivated exploration and hierarchical representation, are more closely related than is usually acknowledged. Implemented jointly, they can lead to an understanding of the environment that goes beyond and is more flexible than, say, the mere prediction of reward as in traditional RL frameworks (Sutton \& Barto). We argue that by combining intrinsically-motivated exploration with hierarchy, agents can discover meaningful patterns of behavior without explicit training, and learn to form multi-step plans without the need of a predictive model. Related work has suggested that this is a valuable approach (Work on novelty: Deepak Pathak, Pulkit Arawal, Yael Niv, Schmidhuber; on hierarchy: Marlos Machado, Botvinick, Yael Niv, Pedro Tsividis; combination of both: Deep HRL paper)

We propose an algorithm that explores an environment of sparse rewards based on its intrinsic curiosity, which is triggered by novel events. Once an event triggers the agent's curiosity (imagine an infant playing with a toy that suddenly makes a new sound), the agent will try to reproduce the event (try to reproduce the sound). Crucially, the agent does not know yet how to produce the event that it is curious about. In order to achieve its goal, it therefore has to learn, through trial and error, a specific option policy that leads to the event. In this way, the agent will learn policies to control various aspects of its environment. We model the process of option creation through hierarchical reinforcement learning, relying on the options framework (Sutton, Precup, Singh). Crucially, the only learning signal hereby is the agent's internal reward upon achieving its self-selected goal.

Over the long term, option policies become better and better at producing their goal events, such that the corresponding events can be produced more frequently and, over time, decrease in novelty. This leads to a decrease in the agent's curiosity, and its intrinsic motivation to explore events further once they are understood. Nevertheless, the world is full of wonders. Whereas an infant might be curious about individual sounds, a toddler might be interested in children's songs, and an adult in twelve-tone music or Italian Opera. Acquiring some skills, rather than reducing motivation and slowing down exploration, opens up new possibilities of learning even more abstract skills. In our framework, basic actions are the building blocks for only the most fundamental option policies. More abstract option policies can be created by using option policies as the building blocks; this process can be repeated indefinitely, resulting in option policies of increasing abstraction, limited only by the temporal horizon of the agent. In other words, the agent can learn to achieve abstract goals by breaking them down into sub-goals and sub-sub-goals, etc.


\section{Methods}

\subsection{Event novelty and curiosity}

Concretely, the agent is exploring an environment that produces certain events in response to the agent's actions (see below for more details). Here, each event is defined as the change in one (binary) feature of the environment from 0 to 1. The number of times each event has occurred determines the event's novelty. Event novelty determines the agent's curiosity about the event, which in turn determines the agent's propensity to select the event as a target of exploration, i.e., to select the sub-goal of producing the event.

Formally, novelty $n_e = e^{-\lambda i_e}$; $\lambda$ is the agent's rate of novelty decay (in the experiments shown below, $\lambda = 0.3$) and $i_e$ is the number times event $e$ has occurred.
Curiosity $c_e$ is updated according to $c_e = c_e + \alpha (\gamma^{j_e} n_e - c_e)$ whenever the event occurs; $\alpha$ is the agent's learning rate ($\alpha = 0.3$); $\gamma$ is the agent's discounting of the future ($\gamma = 0.9$), and $j_e$ is the number of time steps spent within option $o_e$, which led to event $e$ ($j_e = 0$ when event $e$ happened without executing option $o_e$).
In order to select sub-goals, the agent uses $\epsilon$-greedy selection based on its curiosity about all events $e$.

\subsection{Option policies}

When the agent selects a specific event $e$ as sub-goal, the option policy $o_e$ targeted at this event starts controlling action selection. The values of the option policy are updated in each step through TD learning, and eventually reflect which actions lead to the sub-goal. After the termination of an option policy, the agent's curiosity about the sub-goal is updated based on the novelty of the produced event (or the failure to do so).

Formally, option $o_i$ has the option policy $\pi_o$, which is the $\epsilon$-greedy action selection based on option values $V_o$. $V_o$ are defined by the weights $\theta$ of the binary features $\phi$ that describe the state of the environment, $V_o = \theta^T \phi$. In our case, the features indicate which events have happened previously (see below for details).
$V_o$ are updated through temporal difference learning on the weights $\theta$: $\theta = \theta + \alpha (r + \gamma max_a(V_o') - V_o(a))$; $V_o' = \theta^T \phi'$; the pseudo-reward $r = 1$ when $e$ occurred (and the option terminates) and $r = 0$ otherwise; $V_o' = 0$ when the option terminates. 

\subsection{The task: States, actions, and transitions}

- add the figure from my slides that explains how the task works?

We aimed to create a task that is hierarchical in nature and that can easily be presented to human participants in a laboratory setting. The goal was to create an environment with a basic hierarchical structure, as can be found in more naturalistic settings, such as language or motor learning. Language, for example, has a remarkably hierarchical structure in that phonemes are combined into syllables, syllables into words, words into phrases, phrases into sentences, etc.

In our task, an agent can execute a small number of "basic actions" and the environment responds with specific events, according to the following rules. Each basic action deterministically produces a specific "basic event", e.g., executing action $a_3$ produces event $e_3$. In addition, certain sequences of basic actions trigger "level-1" events, e.g., executing $a_3$ then $a_2$ might produce event $e_2$. Similarly, certain sequences of level-1 events produce level-2 events, and so on.

Formally, let $A_0 = a_1, a_2, \ldots, a_k$ be the set of the agent's $k$ basic actions and let $S = (F_{0_1}, F_{0_2}, \ldots, F_{0_k}, F_{1_1}, F_{1_2}, \ldots, F_{1_k}, F_{2_1}, \ldots)$ be the parameterized state space defined by $l \times k$ binary features, where $k$ is the number of basic actions and $l$ is the number of hierarchical levels. 
The deterministic transition between the initial state $s_0$ and $s'$ upon execution of $a_i, i \in [1, \ldots, k]$ is then defined by $s_0 = (f_{m_n} = 0), m, n = [1, \ldots, k]$ and $s' = (f_{0_i} = 1, f_{m_j} = 0), m = [1, \ldots, k], j = [1, \ldots, i-1, i+1, \ldots, k]$. In words, action $a_i$ changes only one feature of the state, feature $f_{0_i}$. We define event $e_i$ as the change from a state in which $f_i = 0$ to a state in which $f_i = 1$. The execution of $o_l = a_l, l \in [1, \ldots, k]$ therefore results in the event $e_l$ and the transition from $s'$ to $s'' = (f_{0_l} = 1, f_{m_j} = 0), m = [1, \ldots, k], j = [1, \ldots, l-1, l+1, \ldots, k]$. Note that at level, one one feature is set to 1 at a time.  The agent's initial option space $O$ comprises only basic actions, $O = A_0$.

The transition function has the following additional property. A small selection of state-action combinations deterministically leads to more than just the corresponding level-0 event $e_{o_n}$. An additional event $e_{1_c}$ will occur if this is determined by the combination of features $f_{0_n}$ and action $a_i$. Similarly, if event $e_{1_c}$ occurred, a level-2 event $e_{2_p}$ will occur if this is determined by the combination of features $f_{1_n}$ and event $e_{1_c}$. The same is true for all levels of the hierarchy. 

More details about our specific implementation: $n$ is identical at all levels; the number of possible events is identical at all levels; rules are allowed to overlap, but cannot be totally identical). E.g., one rule might be [01] (event 0 followed by event 1), and another might be [21].

\subsection{Option creation}

Initially, the agent possesses only "basic actions", $O = A_0$. When a level-1 event occurs for the first time, it is added to the agent's option space as a potential sub-goal. The option policy of the new sub-goal can use all basic actions. Similarly, "level-2" events are triggered by specific sequences of level-1 events; their action space comprises all level-1 options; and so on.

When an event occurs for the first time, the agent's option space is extended by the option $o_e$, which is targeted at producing event $e$: $O = O \cup o_e$. Each option $o_e$ has the whole state space as initiation set, $I = S$. The termination probability $\beta = 1$ for all states in which $f_e = 1$, i.e., in which the target event $e$ occurred, and $\beta = 0.1$ in all other states. The action space of $o_e$ comprises all options $o_{i-1}$ at the level below $o_e$. For example, if $o_1$ is a level-1 option, its action space comprises all basic actions $a \in A_0$. 

\subsection{Final remarks about the agent and environment}

Because options are strictly hierarchical in this way, time scales differ between options at different levels. Level-1 option policies are updated at each time step (because the agent executes one basic action at each time step), but level-2 option policies are only updated after the termination of each level-1 option. In the experiments presented below, all events consist of two events at the level below. Therefore level-2 option policies can be updated every other time step at most, level-3 options every fourth, level-4 option every 8th, and so on. The environment still has the Markov property, even though its workings are defined by sequences of events at varying time scales. 

Note: The current algorithm is tabular, i.e., all states, events, and actions are discrete, and state-action values are represented as tables, and are not approximated through neural networks.

\subsection{Comparison agents}

We compared the novelty-based hierarchical agent described in the paragraphs above to three other agents. The reward-based hierarchical agent employed options in the same way as the novelty-based hierarchical agent, but used reward $r$ instead of novelty as an update signal for option values, $Q(s, o) = Q(s, o) + \alpha (r - Q(s, o))$. We defined reward as the number events produced in a given trial, i.e., the number of features $f_i$ that changed from 0 to 1. 

The novelty-based flat agent selected options based on curiosity like the novelty-based hierarchical agent, but it did not create or use abstract options. Therefore, $O = A_0$ and $S = S_0$. 

The reward-based flat agent is a combination of these two agents.

\section{Results}

All results shown for agents with $\alpha = 0.3$, $\lambda = 0.3$, $\gamma = 0.7$, $\epsilon = 0.2$, $distraction = 0.1$. All environments had 3 basic actions, and 3 events at each level of the hierarchy. Each event was triggered by the sequential occurrence of 2 specific events at the level below. The environment had 6 levels of hierarchy in an effort to max out the agents. 

\subsection{Behavior of the novelty-based hierarchical agent}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{data/plots/Novelty.png}
	\caption{Example illustration of the task. At the current state, three level-0 lights (blue) are on, all level-1 lights are off (invisible) and all three level-2 lights are on, meaning that they will be replaced with a single level-3 light in this turn. The agent's goal is to turn on "all the lights".}
	\label{TaskFigure}
\end{figure}

\begin{itemize}
	\item Novelty over time: One plot for each level, showing how novelty decreases over time; will be the average of 5 agents who played the same environment [the environment with the rules above].
	\item Final option policies: Final Q-value tables for a few options (one at each level?), averaged over the 5 agents.
	\item Learning policies over time: Change in RMSE over trials, plotted separately for each level (averaging RMSE within levels)
\end{itemize}

\subsection{Comparison of the novelty-based hierarchical agent and other agents}

\begin{itemize}
	\item Number of events per trial over time
	\item Number of discovered events over time
	\item Number of perseverative actions over time (perseveration = same action at least twice in a row)
	\item Size of "DA bursts" over time ("DA burst" = RPE at either novelty or theta level)
	\item Number of times actions are executed, by number of times an action is part of a higher-level event (0, 1, or 2)
\end{itemize}

\section{Conclusion}

We have shown that a novelty-based hierarchical RL agent
\begin{itemize}
	\item explores an environment more thoroughly than a flat agent (discovers more events)
	\item selects actions in a more balanced way (fewer perseverations)
	\item "gets bored" over time, especially  for very simple tasks (decreasing DA bursts; decrease in novelty scales with level)
\end{itemize}

This makes sense and might be similar to humans.

In our notion, the agent learns to "understand" its environment. It is able to predict events and control them according to its own wishes.

This is interesting because usually people think about prediction in terms of model-based (versus model-free) methods, rather than hierarchical methods. But creating hierarchy in this way really is a way to achieve prediction. By creating an event option policy, the agent learns to predict the event. By selecting the option policy, it controls the environment.

"Embodiment" is really important, the possibility of the agent to interact with its environment. Because understanding implies control. Kids love to experiment. Cite stuff on how experimentation helps kids learn? (Fei Shu, Tania Lombrozo?, Alison Gopnik?; active learning literature)

\subsection{Future directions}

It is desirable for a complex cognitive system, like a human, to understand its environment in a way that allows prediction of future states and events. This is because with the ability to predict the results of its own actions, an agent will be able to control its environment, creating the states and events it desires. The prediction and control of future rewards, for example, has been formalized in reinforcement learning theory (\cite{sutton_reinforcement_2017}). In this framework, an agent learns over time how much (cumulative, temporally discounted) reward to expect in the future, given the current state of the environment and the agent's action policy. Given the ability to predict, the agent can then maximize future rewards by adjusting its action policy.

\begin{itemize}
	\item But it's not all about rewards: We want to learn the structure of the environment (predictive model) before we know which states and actions will be rewarding.
	\item Example: When an infant looks at a computer monitor, she probably sees a dark frame, filled with colored rectangles and blinking, moving blobs of various sizes. A computer-savy adult, on the other hand, sees a Mac or Windows machine, a bug in the code, or the need to download a new plugin. The perception of the computer monitor changes dramatically upon interaction with it. Interaction creates a structured model of the computer.
	\item Other examples: language learning: learn phonemes, syllables, words, 2-word sentences, 3-word sentences, whole sentences, poetry; motor learning: learn to grasp, hold, manipulate, ..., play the violin.
	\item Indeed, psychological research of the last century has stressed again and again that humans represent their world hierarchically (Chess study - chunking; old research on expertise; TS; Anderson? Making coffee)
	\item What would a predictive model look like that is not focused on reward? Instead of a flat table that specifies how much reward to expect from each state-action pair, we would have a collection of policies that specify how to reach various states in the environment. The agent can control the environment by selecting whatever sub-goal it likes at the moment. Once rewards are added, it can use the learned structure to maximize rewards. 
	\item introduce HRL
	\item ML / RL advances in this direction: Machado, ... (cite papers that talk about exploration; sparse rewards; old 2-step paper; Schmidhuber; Deepak Pathak; the other papers in dropbox papers and in the old draft)
	\item The big question is now how to find valuable sub-goals. Much research, mostly about bottleneck states in the state space (Botvinick -> check in my powerpoint). But this is unrealistic: an agent does not have access to all the information necessary to calculate which states are bottleneck states. 
	\item The goal of this project is to create an algorithm with human-like behavior. Valuable sug-goals will be identified using the agent's curiosity, i.e., intrinsic motivation (Deepak Pathak's citations; Botvinick paper? -> check powerpoint)
\end{itemize}

\begin{itemize}
	\item Algo itself could be extended: function approximation (neural network) rather than tabular RL; model curiosity in a more sophisticated way: based on prediction rather than event count (Deepak Pathak's citations)
	\item Bring the task to humans and compare
	\item extract RL features and look for them in the brain
	\item Check if we can reproduce specific phenomena related to hierarchical learning. First, does the agent have a learning advantage when it is allowed to explore the environment first (without rewards), before asked to exploit it (add rewards, see how long it takes the agent to find the best strategy, compared to a flat or non-trained agent). Second, is there interference when the agent is trained in one environment and then posed into an environment whose options are just slightly different (reason why second-language speakers never get rid of their accent: it's situated at a low level in the language hierarchy).
	\item Check if there is more on the powerpoint.
	\item Maybe all is really just exploration: Science, in the end, is the task of understanding the world.
\end{itemize}

Taken together, by creating the hierarchical novelty-seeking agent, we hope to show that learning of meaningful behavior can be driven by learning itself, rather than external rewards. In this framework, the learner produces the crucial learning signals itself, by selecting actions that over time reduce novelty. In other words, this learner balances the search for novelty with reducing prediction errors. It explores the environment in a systematic way, forms "hypotheses" and conducts "experiments", in order to learn more. 

\section*{References}

\printbibliography

\end{document}